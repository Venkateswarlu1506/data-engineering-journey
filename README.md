# 🚀 Data Engineering Journey with Python, Azure & Databricks

Welcome to my **Data Engineering Learning Repository**!  
This repo tracks my hands-on learning path in **Python, SQL, APIs, Databases, PySpark, Azure, Databricks, Kafka, Orchestration**, and end-to-end projects.

---

## 📂 Repository Structure

01_python_sql/ → Python + SQL foundations
02_apis/ → API data extraction
03_databases/ → PostgreSQL & MongoDB
04_pyspark/ → Batch + Streaming with PySpark
05_adls_storage/ → Azure Data Lake Storage
06_databricks/ → ETL, Delta Lake, Databricks notebooks
07_synapse/ → Azure Synapse Data Warehouse
08_kafka/ → Apache Kafka streaming
09_orchestration/ → Airflow & Azure Data Factory pipelines
10_projects/ → End-to-End real-world projects

---

## 📌 Learning Roadmap

✅ **Python + SQL (Foundations)**  
✅ **APIs (REST, JSON, Authentication, Data Extraction)**  
✅ **Databases (PostgreSQL + MongoDB)**  
✅ **PySpark (Batch + Stream Processing)**  
✅ **Azure Data Lake (ADLS) + Storage**  
✅ **Databricks (ETL + Delta Lake)**  
✅ **Azure Synapse Analytics**  
✅ **Apache Kafka (Message Queues & Streaming)**  
✅ **Orchestration (Airflow + Azure Data Factory)**  
✅ **End-to-End Projects (Real-World Pipelines)**  

---

## 📊 My Projects

### 🔹 Project 1: API → Databricks → Synapse  
- Extracted data from a **public API**  
- Processed with **PySpark in Databricks**  
- Loaded into **Azure Synapse Analytics**  

### 🔹 Project 2: Real-Time Streaming with Kafka & Spark  
- Kafka producer → sends live data  
- Spark Structured Streaming → transforms in real-time  
- Data stored in **Delta Lake + Synapse**  

---

## 🛠️ Tech Stack

- **Programming**: Python, SQL, PySpark  
- **Databases**: PostgreSQL, MongoDB  
- **Big Data**: Databricks, Delta Lake  
- **Cloud**: Azure Data Lake, Synapse, Azure Data Factory  
- **Streaming**: Apache Kafka  
- **Orchestration**: Apache Airflow, ADF  
- **Version Control**: Git + GitHub  

---

## 📅 Progress Tracker

| Module                | Status | Notes |
|------------------------|--------|-------|
| Python + SQL          | 🔄 In Progress | |
| APIs                  | ⏳ Pending | |
| Databases             | ⏳ Pending | |
| PySpark               | ⏳ Pending | |
| Azure Data Lake       | ⏳ Pending | |
| Databricks            | ⏳ Pending | |
| Synapse               | ⏳ Pending | |
| Kafka                 | ⏳ Pending | |
| Orchestration         | ⏳ Pending | |
| Projects              | ⏳ Pending | |

---

## 📖 About Me
I’m learning **Data Engineering + AI in Data** with a focus on **Azure & Databricks**.  
This repo is my **learning log + portfolio** to track projects, progress, and skills.

---

💡 **Tip**: Update this README every time you finish a milestone → it shows growth over time.
-----------------------------------------------------------------------------------
1. Python Foundations for Data Engineering

Variables, data types, operators

Control structures (if, for, while)

Functions & modules

File handling (CSV, JSON, XML, TXT, Excel)

Error handling & logging (logging module)

Python libraries:

pandas → for data manipulation

numpy → for numerical computations

os, pathlib → for file system interaction

requests → for APIs

json, csv → structured/unstructured data

2. Working with Databases (SQL + Python Integration)

SQL basics: SELECT, INSERT, UPDATE, DELETE

Joins, Aggregations, Window functions

Database design (normalization, primary keys, indexes)

Python connectors:

sqlite3

pyodbc / psycopg2 (Postgres)

sqlalchemy (ORM)

ETL with SQL + Python (extract from DB, transform in Python, load back)

3. Data Formats & Sources

CSV, TSV

JSON, XML

Parquet, ORC (big data formats)

APIs (REST, GraphQL)

Web scraping (BeautifulSoup, Scrapy)

Streaming data (Kafka basics)

4. Data Transformation & Processing

Batch Processing:

Pandas transformations

PySpark (for large datasets)

Dask (parallel computing)

Streaming Processing:

Kafka + Python consumers

Spark Structured Streaming

5. Data Storage Systems

Relational Databases (Postgres, MySQL, SQL Server)

NoSQL Databases:

MongoDB (document-based)

Cassandra (wide-column)

Redis (in-memory)

Data Lakes: Store raw files in S3, Azure Data Lake, GCP Storage

Data Warehouses: Snowflake, BigQuery, Azure Synapse, Redshift

6. Data Pipelines & Orchestration

ETL vs ELT concepts

Workflow orchestration tools:

Apache Airflow

Prefect

Luigi

Scheduling & monitoring pipelines

7. Big Data & Distributed Computing

Hadoop ecosystem (HDFS, MapReduce basics)

Spark (PySpark for transformations, joins, aggregations)

Delta Lake / Lakehouse concepts

8. Cloud & Modern Data Engineering

Azure (Databricks, Data Factory, Synapse)

AWS (Glue, Redshift, S3)

GCP (BigQuery, Dataflow, Pub/Sub)

Docker (containerization)

Kubernetes (for scaling pipelines)

9. Version Control & CI/CD

Git & GitHub/GitLab

Branching, merging, pull requests

CI/CD basics (Jenkins, GitHub Actions, GitLab CI)

10. Data Engineering Best Practices

Data quality & validation

Partitioning & bucketing for big data

Schema evolution

Handling duplicates & missing data

Logging & monitoring (Prometheus, Grafana)

✅ Learning Path Suggestion for You:

Python basics → Pandas/Numpy → File handling

SQL + Python integration

Data formats (CSV, JSON, Parquet)

ETL project with Pandas + SQL

PySpark (batch + streaming)

Orchestration (Airflow/Prefect)

Cloud platform (Databricks on Azure would be perfect for you 🚀)
